{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6545cd",
   "metadata": {},
   "source": [
    "# Выделение маски леса на снимке Sentinel-1 SAR: Тестирование моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78502c46",
   "metadata": {},
   "source": [
    "Блокнот разбит на 4 части: оценка сверточной сети с тремя сверточными слоями, обучение ResNet-7, обучение U-Net и обучение Random Forest. \n",
    "\n",
    "В ходе оценки качества моделей производится предсказание ответов для всего снимка, формирование маски леса для всего снимка, оценка метрик для полученной маски, вычисление разницы между оригинальной и сгенерированной масками, а также экспорт маски в виде растра формата geotiff (tiff, содержащий информацию о пространственном расположении снимка на местности)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f987ea",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5703811",
   "metadata": {},
   "source": [
    "Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef363b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# dl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# classic ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6b099",
   "metadata": {},
   "source": [
    "Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848531e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# расчет метрики Intersection over Union\n",
    "def IoU(tp, fp, fn):\n",
    "    return tp / (tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b836686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление метрик\n",
    "def print_metrics(labels, outputs):\n",
    "    print('Доля залесеных территорий на оригинальной маске')\n",
    "    print(f'{labels[labels == 1].size / labels.size :.2}')\n",
    "    print('Доля залесеных территорий на результате работы модели')\n",
    "    print(f'{outputs[outputs == 1].size / outputs.size :.2}')\n",
    "    \n",
    "    if len(labels.shape) > 1:\n",
    "        labels = labels.flatten()\n",
    "        outputs = outputs.flatten()\n",
    "\n",
    "    print(f\"Matthews correlation coefficient={matthews_corrcoef(labels, outputs):.2}\")\n",
    "    print(f\"ROC_AUC={roc_auc_score(labels, outputs):.2}\")\n",
    "    print(f\"Balanced accuracy score={balanced_accuracy_score(labels, outputs):.2}\")\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, outputs).ravel()\n",
    "    iou = IoU(tp, fp, fn)\n",
    "\n",
    "    print(f\"Intersection over Union = {iou:.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f624353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# геопривязка классифицированного изображения\n",
    "def createGeotiff(out_raster, data, geo_transform, projection):\n",
    "    driver = gdal.GetDriverByName('GTiff')\n",
    "    rows, cols = data.shape\n",
    "    rasterDS = driver.Create(out_raster, cols, rows, 1, gdal.GDT_Byte) \n",
    "    rasterDS.SetGeoTransform(geo_transform)\n",
    "    rasterDS.SetProjection(projection)\n",
    "    band = rasterDS.GetRasterBand(1)\n",
    "    band.WriteArray(data)\n",
    "    rasterDS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7217473",
   "metadata": {},
   "source": [
    "### Загрузка и предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 18RnYJKsWTqHfdYZ1Ej_EY44FpJntrKae # forest_mask\n",
    "!gdown --id 18jTiMnaGNneKwCLaThF4tEZWXKdT5fF5 # sentinel-1 sar image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1404c040",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_path = \"/content/subalos_S1B_20191021_.tif\"\n",
    "mask_path = \"/content/forest_mask_.tif\"\n",
    "\n",
    "image = gdal.Open(raster_path, gdal.GA_ReadOnly)\n",
    "\n",
    "# получаем инфо о пространственной привязке\n",
    "geo_transform = image.GetGeoTransform()\n",
    "projection = image.GetProjectionRef()\n",
    "\n",
    "image_array = image.ReadAsArray()\n",
    "\n",
    "mask = gdal.Open(mask_path, gdal.GA_ReadOnly)\n",
    "mask_array = mask.ReadAsArray().astype(np.int8)\n",
    "\n",
    "image = None \n",
    "mask = None\n",
    "\n",
    "print(image_array.shape)\n",
    "print(mask_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1514de",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_1 = image_array[0]\n",
    "band_2 = image_array[1]\n",
    "\n",
    "band_1 = StandardScaler().fit_transform(band_1)\n",
    "band_2 = StandardScaler().fit_transform(band_2)\n",
    "\n",
    "image_norm = np.stack((band_1, band_2), axis=-1)\n",
    "image_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62775113",
   "metadata": {},
   "source": [
    "Чтобы не забывать границу, после которой можно оценивать метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de1a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_test = 2700"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e0b743",
   "metadata": {},
   "source": [
    "Готовые модели также можно скачать, однако необходимо будет изменить пути на загружаемые модели, потому что они имеют отличные названия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aaef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1toFZ-cKZowAvnZHNWqhzPWF8dgSUORnA # cnn\n",
    "!gdown --id 1tFHnTMZH7z_2K8y9Huv1q4bpuK1xKzZr # resnet\n",
    "!gdown --id 1EF0AO0sWjBHeep6NPjqby8IG7bfrygdl # unet\n",
    "# готовая модель random forest весит 3 ГБ и у меня нет возможности ее загрузить на диск :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8244a9f",
   "metadata": {},
   "source": [
    "Готовые маски также можно скачать и использовать для оценки качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea8cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1GT4_eYq-130UXZXJA30J5D6nTgITqI81 # cnn\n",
    "!gdown --id 12kWhnVvfQ5TbbSkCuMLkymQKY2SpaZKk # resnet\n",
    "!gdown --id 1n1im0gdadqvHKlQb36bYjQ8-FC2IMpwl # unet\n",
    "!gdown --id 1pPidsiMsX19hM3frVq9qIFpwqWq8cMnQ # rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ca233",
   "metadata": {},
   "source": [
    "## Сверточные сети для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938b4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# генератор патчей\n",
    "# так как мы классифицируем центральный пиксел, размер стороны патча должен быть нечетным\n",
    "class Patcher(Dataset):\n",
    "    def __init__(self, image, mask, transform, patch_size):\n",
    "        super().__init__()\n",
    "              \n",
    "        assert patch_size % 2, \"Нечетные патчи, пожалуйста!\"\n",
    "        self.image = image\n",
    "        self.mask = mask\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.im_h, self.im_w = image.shape[0], image.shape[1]\n",
    "    \n",
    "        half_patch = self.patch_size // 2\n",
    "        # координаты центрального пиксела для восстановления маски\n",
    "        coord_list = list()\n",
    "        for central_x in trange(half_patch, self.im_w - half_patch): \n",
    "            for central_y in range(half_patch, self.im_h - half_patch):\n",
    "                # создаем патч, только если он не нулевой\n",
    "                if (self.image[central_y - half_patch:central_y + half_patch + 1,\n",
    "                               central_x - half_patch:central_x + half_patch + 1] != 0).all():\n",
    "                    coord_list.append([central_y, central_x])\n",
    "        self.coords = np.array(coord_list)\n",
    "        self.size = len(self.coords)\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        central_x = self.coords[indx, 1] # на основе координат центрального пиксела\n",
    "        central_y = self.coords[indx, 0]\n",
    "        \n",
    "        half_patch = self.patch_size // 2\n",
    "        # вырезаем патч\n",
    "        patch = self.image[central_y - half_patch:central_y + half_patch + 1, \n",
    "                           central_x - half_patch:central_x + half_patch + 1]\n",
    "        \n",
    "        # определяем класс\n",
    "        label = self.mask[central_y][central_x]\n",
    "        return self.transform(patch), torch.tensor(label), indx \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для валидации на тесте \n",
    "def final_validate(model, \n",
    "                  criterion,\n",
    "                  test_loader):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    coords = []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            patch, label, coord = batch\n",
    "            patch, label = patch.to(device), label.to(device)\n",
    "            y_pred = model(patch) # get predictions\n",
    "            y_pred = y_pred.squeeze().cpu()\n",
    "            y_pred = torch.where(y_pred > 0.5, 1, 0)\n",
    "            outputs.append(y_pred.numpy())\n",
    "            coords.append(coord)\n",
    "        \n",
    "    outputs = np.concatenate(outputs, axis=0)\n",
    "    coords = np.concatenate(coords, axis=0)\n",
    "    \n",
    "    \n",
    "    return outputs, coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5678817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполнение болванки изображения ответами\n",
    "def fill_image(coord, dataset, zero_image, outputs):\n",
    "    for indx in trange(len(coord)):\n",
    "        coord_x, coord_y = dataset.coords[coord[indx]]\n",
    "        zero_image[coord_x, coord_y] = outputs[indx]\n",
    "        \n",
    "    return zero_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b494abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result_image(patch_size, \n",
    "                          img, lbl, \n",
    "                          transform, batch_size, \n",
    "                          model, criterion):\n",
    "    \n",
    "    dataset = Patcher(img, lbl, transform, patch_size) # without noise\n",
    "        \n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "    \n",
    "    outputs, coords = final_validate(model, \n",
    "                                     criterion,\n",
    "                                     loader)\n",
    "\n",
    "    result_image = np.zeros_like(lbl)\n",
    "    \n",
    "    result_image = fill_image(coord=coords, \n",
    "                              dataset=dataset, \n",
    "                              zero_image=result_image, \n",
    "                              outputs=outputs)\n",
    "\n",
    "\n",
    "    result_image[result_image > 1] = 1\n",
    "    \n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16de2b",
   "metadata": {},
   "source": [
    "Так как мы только оцениваем модели, нам не нужны аугментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform = transforms.Compose([\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d71a50",
   "metadata": {},
   "source": [
    "### Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a0659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_s1(nn.Module):\n",
    "    def __init__(self, patch_size: int = 5):\n",
    "        super().__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, 3, stride=1, padding=1), # shape: [32,patch_size,patch_size]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1), # shape: [64,patch_size,patch_size] \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1), # shape: [128,patch_size,patch_size] \n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*patch_size*patch_size, 1000),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(1000, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = self.conv_stack(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e710bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 15\n",
    "batch_size = 2048\n",
    "\n",
    "model_cnn = torch.load('/content/model_cnn.pt')\n",
    "model_cnn.eval()\n",
    "model_cnn = model_cnn.to(device)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4068dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_image = generate_result_image(patch_size=patch_size, \n",
    "                                     img=image_norm, lbl=mask_array, \n",
    "                                     transform=valid_transform, batch_size=batch_size, \n",
    "                                     model=model_cnn, criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf6a37",
   "metadata": {},
   "source": [
    "__Метрики__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Метрики для всего изображения')\n",
    "print()\n",
    "print_metrics(mask_array, result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfda8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Метрики для тестовой части')\n",
    "print()\n",
    "print_metrics(mask_array[bound_test:], result_image[bound_test:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148c674",
   "metadata": {},
   "source": [
    "__Оригинальная и смоделированная маски__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(20, 30))\n",
    "axis[0].imshow(mask_array, cmap='Greys_r')\n",
    "axis[1].imshow(result_image, cmap='Greys_r')\n",
    "axis[0].title.set_text('Original')\n",
    "axis[1].title.set_text('Parody')\n",
    "for a in axis:\n",
    "    a.axis('off') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26555dbb",
   "metadata": {},
   "source": [
    "Фрагмент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaca423",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_x = 3500\n",
    "step_x = 1000\n",
    "start_y = 100\n",
    "step_y = 4900\n",
    "\n",
    "figure1 = mask_array[start_x:start_x + step_x, start_y:start_y + step_y]\n",
    "figure2 = result_image[start_x:start_x + step_x, start_y:start_y + step_y]\n",
    "\n",
    "fig, axis = plt.subplots(2, 1, figsize=(15, 7))\n",
    "axis[0].imshow(figure1, cmap='Greys_r')\n",
    "axis[1].imshow(figure2, cmap='Greys_r')\n",
    "axis[0].title.set_text('Original')\n",
    "axis[1].title.set_text('Parody')\n",
    "for a in axis:\n",
    "    a.axis('off') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94f518f",
   "metadata": {},
   "source": [
    "__Разница между оригинальной маской и смоделированной__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec47dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_mask = mask_array - result_image\n",
    "\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "plt.imshow(diff_mask, cmap='bwr')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7105bbff",
   "metadata": {},
   "source": [
    "Белый - нет разницы\n",
    "\n",
    "Красный - необнаруженный лес\n",
    "\n",
    "Синий - лишний лес"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55737bf5",
   "metadata": {},
   "source": [
    "__Экспорт результата__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f36416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_raster = \"/content/result_cnn.tif\"\n",
    "\n",
    "# экспорт результата\n",
    "createGeotiff(out_raster, result_image, geo_transform, projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7b1a8",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d48913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResnet(nn.Module):\n",
    "    def __init__(self, class_nums = 1, patch_size=5):\n",
    "        super(CustomResnet, self).__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        resnet_module = nn.Sequential(\n",
    "            nn.Conv2d(2, 64, 3, stride=1, padding=1),\n",
    "            self.activation,\n",
    "            BasicBlock(64, 64, 1),\n",
    "            BasicBlock(64, 128, 2),\n",
    "            BasicBlock(128, 128, 1),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "\n",
    "        dummy_imput = torch.rand(1, 2, patch_size, patch_size)\n",
    "        out = resnet_module(dummy_imput)\n",
    "\n",
    "        self.resnet = resnet_module\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(out.shape[1], class_nums),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = nn.Flatten()(x)\n",
    "        scores = self.fc(x)\n",
    "        return scores\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, conv_in, conv_out, stride_first, activation = nn.ReLU):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.activation = activation()\n",
    "\n",
    "        if stride_first == 2:\n",
    "            downs_module = nn.Sequential(\n",
    "            nn.Conv2d(conv_in, conv_out, 1, stride=2),\n",
    "            nn.BatchNorm2d(conv_out)\n",
    "            )\n",
    "            self.downsample = downs_module\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        bb_module = nn.Sequential(\n",
    "            nn.Conv2d(conv_in, conv_out, 3, stride=stride_first, padding=1),\n",
    "            nn.BatchNorm2d(conv_out),\n",
    "            self.activation,\n",
    "            nn.Conv2d(conv_out, conv_out, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(conv_out)\n",
    "        )\n",
    "        self.bb = bb_module\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_identity = x\n",
    "        out = self.bb(x)\n",
    "\n",
    "        if self.downsample is not None: \n",
    "            x_identity = self.downsample(x) \n",
    "\n",
    "        out += x_identity\n",
    "        out = self.activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ec427",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 15\n",
    "batch_size = 2048\n",
    "\n",
    "model_resnet = torch.load('/content/model_resnet.pt')\n",
    "model_resnet.eval()\n",
    "model_resnet = model_resnet.to(device)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10fff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_image = generate_result_image(patch_size=patch_size, \n",
    "                                     img=image_norm, lbl=mask_array, \n",
    "                                     transform=valid_transform, batch_size=batch_size, \n",
    "                                     model=model_resnet, criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142bdb15",
   "metadata": {},
   "source": [
    "__Метрики__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe625a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Метрики для всего изображения')\n",
    "print()\n",
    "print_metrics(mask_array, result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Метрики для тестовой части')\n",
    "print()\n",
    "print_metrics(mask_array[bound_test:], result_image[bound_test:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b878b29",
   "metadata": {},
   "source": [
    "__Оригинальная и смоделированная маски__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c76d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(20, 30))\n",
    "axis[0].imshow(mask_array, cmap='Greys_r')\n",
    "axis[1].imshow(result_image, cmap='Greys_r')\n",
    "axis[0].title.set_text('Original')\n",
    "axis[1].title.set_text('Parody')\n",
    "for a in axis:\n",
    "    a.axis('off') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40129e2",
   "metadata": {},
   "source": [
    "Фрагмент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c38e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_x = 3500\n",
    "step_x = 1000\n",
    "start_y = 100\n",
    "step_y = 4900\n",
    "\n",
    "figure1 = mask_array[start_x:start_x + step_x, start_y:start_y + step_y]\n",
    "figure2 = result_image[start_x:start_x + step_x, start_y:start_y + step_y]\n",
    "\n",
    "fig, axis = plt.subplots(2, 1, figsize=(15, 7))\n",
    "axis[0].imshow(figure1, cmap='Greys_r')\n",
    "axis[1].imshow(figure2, cmap='Greys_r')\n",
    "axis[0].title.set_text('Original')\n",
    "axis[1].title.set_text('Parody')\n",
    "for a in axis:\n",
    "    a.axis('off') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af1e85f",
   "metadata": {},
   "source": [
    "__Разница между оригинальной маской и смоделированной__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_mask = mask_array - result_image\n",
    "\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "plt.imshow(diff_mask, cmap='bwr')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb09d4b",
   "metadata": {},
   "source": [
    "Белый - нет разницы\n",
    "\n",
    "Красный - необнаруженный лес\n",
    "\n",
    "Синий - лишний лес"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d7192",
   "metadata": {},
   "source": [
    "__Экспорт результата__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17886262",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_raster = \"/content/result_resnet.tif\"\n",
    "\n",
    "# экспорт результата\n",
    "createGeotiff(out_raster, result_image, geo_transform, projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53dd066",
   "metadata": {},
   "source": [
    "## Сверточная сеть U-Net для сегментации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258011ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new patches\n",
    "class Patcher_UNet(Dataset):\n",
    "    def __init__(self, image, mask, transform, patch_size=256, train_part=True):\n",
    "        super().__init__()\n",
    "              \n",
    "        self.image = image\n",
    "        self.mask = mask\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.im_h, self.im_w = image.shape[0], image.shape[1]\n",
    "        self.train_part = train_part\n",
    "    \n",
    "        coord_list = list()\n",
    "        for corner_x in trange(0, self.im_w // self.patch_size * self.patch_size, self.patch_size): \n",
    "            for corner_y in range(0,  self.im_h // self.patch_size * self.patch_size, self.patch_size):\n",
    "                if (self.image[corner_y:corner_y + self.patch_size,\n",
    "                               corner_x:corner_x + self.patch_size] != 0).all():\n",
    "                    coord_list.append([corner_y, corner_x])\n",
    "        if not train_part:\n",
    "            corner_x = self.im_w - self.patch_size\n",
    "            for corner_y in range(0,  self.im_h // self.patch_size * self.patch_size, self.patch_size):\n",
    "                coord_list.append([corner_y, corner_x])\n",
    "            \n",
    "        self.coords = np.array(coord_list)\n",
    "        self.size = len(self.coords)\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        corner_x = self.coords[indx, 1]\n",
    "        corner_y = self.coords[indx, 0]\n",
    "        \n",
    "        patch = self.image[corner_y:corner_y + self.patch_size, \n",
    "                           corner_x:corner_x + self.patch_size]\n",
    "        label = self.mask[corner_y:corner_y + self.patch_size, \n",
    "                           corner_x:corner_x + self.patch_size]\n",
    "        \n",
    "        if self.train_part:\n",
    "            trans = transforms.Compose([transforms.ToTensor()])\n",
    "            concat = torch.cat((trans(patch), trans(label)))\n",
    "            concat_transformed = self.transform(concat)\n",
    "            patch, label = torch.split(concat_transformed, 2)\n",
    "        else:\n",
    "            patch, label = self.transform(patch), self.transform(label)\n",
    "        return patch, label, indx # dataset.coords[indx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b89978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполнение болванки изображения ответами\n",
    "def fill_image_Unet(coord, dataset, zero_image, outputs, patch_size):\n",
    "    for indx in trange(len(coord)):\n",
    "        coord_x, coord_y = dataset.coords[coord[indx]]\n",
    "        zero_image[coord_x:coord_x + patch_size, coord_y:coord_y + patch_size] = outputs[indx]\n",
    "        \n",
    "    return zero_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result_image_unet(patch_size, \n",
    "                          img, lbl, \n",
    "                          transform, batch_size, \n",
    "                          model, criterion):\n",
    "    \n",
    "    dataset = Patcher_UNet(img, lbl, transform, patch_size, train_part = False) # without noise\n",
    "        \n",
    "    loader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=False)\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    coords = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            imgs, labels, coord = batch\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            y_pred = model(imgs)\n",
    "            y_pred = y_pred.squeeze().cpu()\n",
    "            y_pred = torch.where(y_pred > 0.5, 1, 0)\n",
    "            outputs.append(y_pred.numpy())\n",
    "            coords.append(coord)\n",
    "    outputs = np.concatenate(outputs, axis=0)\n",
    "    coords = np.concatenate(coords, axis=0)\n",
    "        \n",
    "\n",
    "    result_image = np.zeros_like(lbl)\n",
    "    \n",
    "    result_image = fill_image_Unet(coord=coords, \n",
    "                                   dataset=dataset, \n",
    "                                   zero_image=result_image, \n",
    "                                   outputs=outputs,\n",
    "                                   patch_size=patch_size)\n",
    "\n",
    "\n",
    "    result_image[result_image > 1] = 1\n",
    "    \n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "class BinaryDiceLoss(nn.Module):\n",
    "    def __init__(self, p=2, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = p  # pow degree\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        predict = predict.flatten(1)\n",
    "        target = target.flatten(1)\n",
    "\n",
    "        num = torch.sum(torch.mul(predict, target), dim=1) + self.epsilon\n",
    "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.epsilon\n",
    "        loss = 1 - 2 * num / den\n",
    "\n",
    "        return loss.mean()  # over batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b98e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_transform = transforms.Compose([\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3378222",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 256\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcbf720",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unet = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                       in_channels=2, out_channels=1, init_features=32, pretrained=False)\n",
    "model_unet = torch.load(\"/model_unet.pkl\")\n",
    "model_unet.eval()\n",
    "model_unet = model_unet.to(device)\n",
    "criterion = BinaryDiceLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae3815",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_image = generate_result_image_unet(patch_size=patch_size, \n",
    "                                         img=image_norm, lbl=mask_array, \n",
    "                                         transform=valid_transform, batch_size=batch_size, \n",
    "                                         model=model_unet, criterion=criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6861ff1",
   "metadata": {},
   "source": [
    "__Метрики__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b3dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Метрики для всего изображения')\n",
    "print()\n",
    "print_metrics(mask_array, result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a47c373",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Метрики для тестовой части')\n",
    "print()\n",
    "print_metrics(mask_array[bound_test:], result_image[bound_test:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f5795",
   "metadata": {},
   "source": [
    "__Оригинальная и смоделированная маски__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf8139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(20, 30))\n",
    "axis[0].imshow(mask_array, cmap='Greys_r')\n",
    "axis[1].imshow(result_image, cmap='Greys_r')\n",
    "axis[0].title.set_text('Original')\n",
    "axis[1].title.set_text('Parody')\n",
    "for a in axis:\n",
    "    a.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a1590",
   "metadata": {},
   "source": [
    "Фрагмент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37a85ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_x = 3500\n",
    "step_x = 1000\n",
    "start_y = 100\n",
    "step_y = 4900\n",
    "\n",
    "figure1 = mask_array[start_x:start_x + step_x, start_y:start_y + step_y]\n",
    "figure2 = result_image[start_x:start_x + step_x, start_y:start_y + step_y]\n",
    "\n",
    "fig, axis = plt.subplots(2, 1, figsize=(15, 7))\n",
    "axis[0].imshow(figure1, cmap='Greys_r')\n",
    "axis[1].imshow(figure2, cmap='Greys_r')\n",
    "axis[0].title.set_text('Original')\n",
    "axis[1].title.set_text('Parody')\n",
    "for a in axis:\n",
    "    a.axis('off') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c876f374",
   "metadata": {},
   "source": [
    "__Разница между оригинальной маской и смоделированной__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addf63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_mask = mask_array - result_image\n",
    "\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "plt.imshow(diff_mask, cmap='bwr')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f8e06",
   "metadata": {},
   "source": [
    "Белый - нет разницы\n",
    "\n",
    "Красный - необнаруженный лес\n",
    "\n",
    "Синий - лишний лес"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42b3c8",
   "metadata": {},
   "source": [
    "__Экспорт результата__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07e9ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_raster = \"/content/result_unet.tif\"\n",
    "\n",
    "# экспорт результата\n",
    "createGeotiff(out_raster, result_image, geo_transform, projection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdced1ce",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd559ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = joblib.load(\"/content/random_forest.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30619b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_image_rf = np.reshape(image_norm, [image_norm.shape[0] * image_norm.shape[1], image_norm.shape[2]])\n",
    "full_labels_rf = mask_array.flatten()\n",
    "\n",
    "result_image = rf.predict(full_image_rf)\n",
    "result_image = np.reshape(result_image, (image_norm.shape[0], image_norm.shape[1]))\n",
    "result_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f96e93",
   "metadata": {},
   "source": [
    "__Метрики__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Метрики для всего изображения')\n",
    "print()\n",
    "print_metrics(mask_array, result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb723cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Метрики для тестовой части')\n",
    "print()\n",
    "print_metrics(mask_array[bound_test:], result_image[bound_test:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8400f56",
   "metadata": {},
   "source": [
    "__Оригинальная и смоделированная маски__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(20, 30))\n",
    "axis[0].imshow(mask_array, cmap='Greys_r')\n",
    "axis[1].imshow(result_image, cmap='Greys_r')\n",
    "axis[0].title.set_text('Original')\n",
    "axis[1].title.set_text('Parody')\n",
    "for a in axis:\n",
    "    a.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f27c43",
   "metadata": {},
   "source": [
    "Фрагмент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0fd627",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_x = 3500\n",
    "step_x = 1000\n",
    "start_y = 100\n",
    "step_y = 4900\n",
    "\n",
    "figure1 = mask_array[start_x:start_x + step_x, start_y:start_y + step_y]\n",
    "figure2 = result_image[start_x:start_x + step_x, start_y:start_y + step_y]\n",
    "\n",
    "fig, axis = plt.subplots(2, 1, figsize=(15, 7))\n",
    "axis[0].imshow(figure1, cmap='Greys_r')\n",
    "axis[1].imshow(figure2, cmap='Greys_r')\n",
    "axis[0].title.set_text('Original')\n",
    "axis[1].title.set_text('Parody')\n",
    "for a in axis:\n",
    "    a.axis('off') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa39a77",
   "metadata": {},
   "source": [
    "__Разница между оригинальной маской и смоделированной__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de7b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_mask = mask_array - result_image\n",
    "\n",
    "fig = plt.figure(figsize=(10, 15))\n",
    "plt.imshow(diff_mask, cmap='bwr')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014b78a2",
   "metadata": {},
   "source": [
    "Белый - нет разницы\n",
    "\n",
    "Красный - необнаруженный лес\n",
    "\n",
    "Синий - лишний лес"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609161d6",
   "metadata": {},
   "source": [
    "__Экспорт результата__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29428568",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_raster = \"/content/result_rf.tif\"\n",
    "\n",
    "# экспорт результата\n",
    "createGeotiff(out_raster, result_image, geo_transform, projection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpytorch",
   "language": "python",
   "name": "envpytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
