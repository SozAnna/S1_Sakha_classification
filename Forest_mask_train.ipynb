{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229de397",
   "metadata": {},
   "source": [
    "ПОПРАВИТЬ ПУТИ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca67cc",
   "metadata": {},
   "source": [
    "# Выделение маски леса на снимке Sentinel-1 SAR: Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe72f150",
   "metadata": {},
   "source": [
    "Блокнот разбит на 4 части: обучение сверточной сети с тремя сверточными слоями, обучение ResNet-7, обучение U-Net и обучение Random Forest. \n",
    "\n",
    "Оценка качества моделей и формирование выходной маски представлены в соответствующем блокноте.\n",
    "\n",
    "В качестве предупреждения: быстрее всего происходит обучение U-Net и может занимать в районе 20 минут, далее Random Forest (около 30 минут), дольше всего учатся CNN и ResNet (в зависимости от размера патча, но можно ориентироваться на 1,5 ч)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9b90c",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee0294a",
   "metadata": {},
   "source": [
    "Загрузка библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13130004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# dl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# classic ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, matthews_corrcoef\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0812eebb",
   "metadata": {},
   "source": [
    "Функции, которые будут использоваться во всех моделях. Функции, которые относятся к конкретным моделям, вынесены в соответствующие главы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0837b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подсчет статистик для каналов изображения\n",
    "def count_statistics(x):\n",
    "    x_min = np.min(x)\n",
    "    x_max = np.max(x)\n",
    "    x_mean = np.mean(x)\n",
    "    x_std = np.std(x)\n",
    "    print(f\"min={x_min:.2}  max={x_max:.2}  mean={x_mean:.2}  std={x_std:.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# расчет метрики Intersection over Union\n",
    "def IoU(tp, fp, fn):\n",
    "    return tp / (tp + fp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52342c",
   "metadata": {},
   "source": [
    "### Загрузка и предобработка\n",
    "\n",
    "Задаем пути на изображение, для которого будет проводиться классификация, и на маску (label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d38309",
   "metadata": {},
   "source": [
    "Открытие изображений, сохранение информации о пространственной привязке для будущего выходного изображения, преобразование в матрицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69629438",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 18RnYJKsWTqHfdYZ1Ej_EY44FpJntrKae # forest_mask\n",
    "!gdown --id 18jTiMnaGNneKwCLaThF4tEZWXKdT5fF5 # sentinel-1 sar image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2e320",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_path = \"/content/subalos_S1B_20191021_.tif\"\n",
    "mask_path = \"/content/forest_mask_.tif\"\n",
    "\n",
    "image = gdal.Open(raster_path, gdal.GA_ReadOnly)\n",
    "\n",
    "# получаем инфо о пространственной привязке\n",
    "geo_transform = image.GetGeoTransform()\n",
    "projection = image.GetProjectionRef()\n",
    "\n",
    "image_array = image.ReadAsArray()\n",
    "\n",
    "mask = gdal.Open(mask_path, gdal.GA_ReadOnly)\n",
    "mask_array = mask.ReadAsArray().astype(np.int8)\n",
    "\n",
    "image = None \n",
    "mask = None\n",
    "\n",
    "print(image_array.shape)\n",
    "print(mask_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb933a5e",
   "metadata": {},
   "source": [
    "Наш снимок в пикселах имеет размер 6146 на 5008, а также содержит 2 канала: это две поляризации VH и VV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e03dcdc",
   "metadata": {},
   "source": [
    "Нужно нормализовать каналы, поскольку известно, что радиолокационные сигналы достаточно низкие, а сигнал в кросс-поляризации ниже, чем в согласованной (из-за смены поляризации происходит потеря энергии)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e12fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(15, 7))\n",
    "axis[0].boxplot([image_array[0].flatten(), image_array[1].flatten()])\n",
    "axis[1].boxplot([image_array[0].flatten(), image_array[1].flatten()])\n",
    "axis[1].set_ylim((-0.01, 0.15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb953490",
   "metadata": {},
   "source": [
    "На графике ящиков с усами мы видим, какое влияние оказывает спекл-шум на значения радиолокационных сигналов. Слева представлен график ящиков с усами без ограничений, справа - с приближением к межквартильному размаху. Мы можем видеть, что сигналы в кросс-поляризации действительно ниже, чем в вертикальной согласованной, а также что шумные пикселы существенно превышают в значениях значения, характерные для снимков.\n",
    "\n",
    "Поскольку речь идет об одном снимке, мы можем нормировать его полностью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "band_1 = image_array[0]\n",
    "band_2 = image_array[1]\n",
    "\n",
    "print('first channel')\n",
    "count_statistics(band_1)\n",
    "print()\n",
    "print('second channel')\n",
    "count_statistics(band_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b62483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard scaler для CNN\n",
    "band_1 = (band_1 - np.mean(band_1)) / np.std(band_1)\n",
    "band_2 = (band_2 - np.mean(band_2)) / np.std(band_2)\n",
    "\n",
    "# standard scaler для ResNet, U-Net и Random Forest\n",
    "band_1 = StandardScaler().fit_transform(band_1)\n",
    "band_2 = StandardScaler().fit_transform(band_2)\n",
    "\n",
    "print('first channel')\n",
    "count_statistics(band_1)\n",
    "print()\n",
    "print('second channel')\n",
    "count_statistics(band_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb41c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(1, 2, figsize=(15, 7))\n",
    "axis[0].boxplot([band_1.flatten(), band_2.flatten()])\n",
    "axis[1].boxplot([band_1.flatten(), band_2.flatten()])\n",
    "axis[1].set_ylim((-2.5, 2.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4516046",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_norm = np.stack((band_1, band_2), axis=-1)\n",
    "image_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d18ddf9",
   "metadata": {},
   "source": [
    "Визуализируем фрагмент изображений: VH-поляризацию, VV-поляризацию и маску леса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3b9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# образцово-показательный кусочек\n",
    "step = 100\n",
    "figure1 = image_norm[1000:1000+step, 600:600+step, :1] \n",
    "figure2 = image_norm[1000:1000+step, 600:600+step, 1:] \n",
    "figure3 = mask_array[1000:1000+step, 600:600+step]\n",
    "\n",
    "fig, axis = plt.subplots(1, 3)\n",
    "axis[0].imshow(figure1, cmap='Greys_r')\n",
    "axis[1].imshow(figure2, cmap='Greys_r')\n",
    "axis[2].imshow(figure3, cmap='Greys_r')\n",
    "for a in axis:\n",
    "    a.axis('off') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d776fb45",
   "metadata": {},
   "source": [
    "Можно обратить внимание на то, что на снимке не_лес темнее.\n",
    "\n",
    "Посмотрим на долю лесных пикселов среди всех пикселов в маске:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e60790",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{mask_array[mask_array == 1].size / mask_array.size :.2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f4793",
   "metadata": {},
   "source": [
    "Доля 63 %, значит, наблюдается небольшой дисбаланс."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15c64bd",
   "metadata": {},
   "source": [
    "## Классификация сверточными сетями\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46e994f",
   "metadata": {},
   "source": [
    "**Функции**, которые будут использоваться для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84316b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# генератор патчей\n",
    "# так как мы классифицируем центральный пиксел, размер стороны патча должен быть нечетным\n",
    "class Patcher(Dataset):\n",
    "    def __init__(self, image, mask, transform, patch_size):\n",
    "        super().__init__()\n",
    "              \n",
    "        assert patch_size % 2, \"Нечетные патчи, пожалуйста!\"\n",
    "        self.image = image\n",
    "        self.mask = mask\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.im_h, self.im_w = image.shape[0], image.shape[1]\n",
    "    \n",
    "        half_patch = self.patch_size // 2\n",
    "        # координаты центрального пиксела для восстановления маски\n",
    "        coord_list = list()\n",
    "        for central_x in trange(half_patch, self.im_w - half_patch): \n",
    "            for central_y in range(half_patch, self.im_h - half_patch):\n",
    "                # создаем патч, только если он не нулевой\n",
    "                if (self.image[central_y - half_patch:central_y + half_patch + 1,\n",
    "                               central_x - half_patch:central_x + half_patch + 1] != 0).all():\n",
    "                    coord_list.append([central_y, central_x])\n",
    "        self.coords = np.array(coord_list)\n",
    "        self.size = len(self.coords)\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        central_x = self.coords[indx, 1] # на основе координат центрального пиксела\n",
    "        central_y = self.coords[indx, 0]\n",
    "        \n",
    "        half_patch = self.patch_size // 2\n",
    "        # вырезаем патч\n",
    "        patch = self.image[central_y - half_patch:central_y + half_patch + 1, \n",
    "                           central_x - half_patch:central_x + half_patch + 1]\n",
    "        \n",
    "        # определяем класс\n",
    "        label = self.mask[central_y][central_x]\n",
    "        return self.transform(patch), torch.tensor(label), indx \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbdb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для валидации\n",
    "def validate(model,\n",
    "             criterion,\n",
    "             val_loader):\n",
    "    cumloss = 0\n",
    "    loss_history = []\n",
    "    mcc = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            x_train, y_train, coords = batch\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "            y_pred = model(x_train) # get predictions\n",
    "            loss = criterion(y_pred.squeeze(), y_train.to(torch.float32)) # compute loss\n",
    "            loss_history.append(loss.cpu().detach().numpy()) # write loss to log\n",
    "            cumloss += loss\n",
    "            \n",
    "            # оценка коэффициента Мэттьюса во время обучения\n",
    "            y_pred = y_pred.squeeze().cpu()\n",
    "            y_pred = torch.where(y_pred > 0.5, 1, 0)\n",
    "            mcc_batch = matthews_corrcoef(y_train.to(torch.float32).cpu(), y_pred)\n",
    "            mcc.append(mcc_batch)\n",
    "            \n",
    "    return cumloss / len(val_loader), loss_history, np.mean(mcc) # mean loss and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aa7103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для обучения\n",
    "def train(model, train_data, test_data, criterion, optimizer, num_epochs=10):\n",
    "    loss_hist = []\n",
    "    val_loss_lst = []\n",
    "    epochs = trange(num_epochs)\n",
    "    for epoch in epochs:\n",
    "        ep_loss = 0\n",
    "        model.train() # dropout!\n",
    "        for batch in train_data:\n",
    "            imgs, labels, coords = batch\n",
    "            imgs, labels = imgs.to(device), labels.to(device)     \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs.squeeze(), labels.to(torch.float32))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ep_loss += loss.item()\n",
    "        loss_hist.append(ep_loss / len(train_data))\n",
    "        \n",
    "        model.eval() # dropout!\n",
    "        val_loss, val_loss_hist, mcc = validate(model, criterion, test_data)\n",
    "        val_loss_lst.append(val_loss.cpu())\n",
    "        \n",
    "        print(f\"Epoch={epoch}  loss={loss_hist[epoch]:.4}  val_loss={val_loss:.4}  mcc={mcc:.2}\")\n",
    "    return val_loss_lst, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27626225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для валидации обученной модели\n",
    "# возвращает предсказания и их координаты\n",
    "# печатает метрики для валидационного набора\n",
    "def final_validate(model, \n",
    "                  criterion,\n",
    "                  val_loader):\n",
    "    model.eval()\n",
    "    cumloss = 0\n",
    "    labels = []\n",
    "    outputs = []\n",
    "    coords = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            patch, label, coord = batch\n",
    "            patch, label = patch.to(device), label.to(device)\n",
    "            y_pred = model(patch) # get predictions\n",
    "            loss = criterion(y_pred.squeeze(), label.to(torch.float32)) # compute loss\n",
    "            cumloss += loss\n",
    "            y_pred = y_pred.squeeze().cpu()\n",
    "            y_pred = torch.where(y_pred > 0.5, 1, 0)\n",
    "            outputs.append(y_pred.numpy())\n",
    "            labels.append(label.cpu().numpy())\n",
    "            coords.append(coord)\n",
    "        \n",
    "    cumloss = cumloss / len(test_loader)\n",
    "    outputs = np.concatenate(outputs, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    coords = np.concatenate(coords, axis=0)\n",
    "    \n",
    "    print(f\"loss={cumloss:.2}\")\n",
    "    print(f\"matthews_correlation_coefficient={matthews_corrcoef(labels, outputs):.2}\")\n",
    "    print(f\"ROC_AUC={roc_auc_score(labels, outputs):.2}\")\n",
    "    print(f\"Balanced accuracy score={balanced_accuracy_score(labels, outputs):.2}\")\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, outputs).ravel()\n",
    "    iou = IoU(tp, fp, fn)\n",
    "    print(f\"Intersection over Union = {iou:.2}\")\n",
    "    \n",
    "    return outputs, coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2410878b",
   "metadata": {},
   "source": [
    "Настало время резать на **train-val-test**. Так как снимок содержит порядка 30 млн пикселов, будет иметь достаточно много патчей суммарно (в случае с попиксельной классификацией при помощи сверточной сети - 30 млн патчей), что приводит к существенным временн**ы**м затратам при обучении. По этой причине в исследовании для обучения использовалось только 17 % изображения в качестве тренировочной части, 3 % в качестве валидационной, остальная часть - тестовая. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba7f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# верхняя граница тренировочных данных\n",
    "bound_train = 1500\n",
    "# верхняя граница валидационных данных\n",
    "bound_val = 2500\n",
    "# нижняя граница валидационных данных\n",
    "bound_test = 2700\n",
    "\n",
    "train_image = image_norm[bound_train:bound_val]\n",
    "train_labels = mask_array[bound_train:bound_val]\n",
    "\n",
    "val_image = image_norm[bound_val:bound_test]\n",
    "val_labels = mask_array[bound_val:bound_test]\n",
    "\n",
    "print(train_image.shape) # 6146 rows total\n",
    "print(train_labels.shape)\n",
    "print(val_image.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c5765",
   "metadata": {},
   "source": [
    "**Подготовка данных к загрузке в модель**\n",
    "\n",
    "Зададим трансформации. Мы хотим добавить шум в тренировочные данные, чтобы модель не учила шум в данных, но шума среди аугментаций в Pytorch нет. Добавим его самостоятельно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7228784",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a099094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.2)])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4e0ddf",
   "metadata": {},
   "source": [
    "Режем на патчи. Готовим датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180675b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 15\n",
    "train_dataset = Patcher(train_image, train_labels, train_transform, patch_size)\n",
    "valid_dataset = Patcher(val_image, val_labels, valid_transform, patch_size)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730cd8b7",
   "metadata": {},
   "source": [
    "### Simple CNN\n",
    "\n",
    "Собираем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90520000",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_s1(nn.Module):\n",
    "    def __init__(self, patch_size: int = 5):\n",
    "        super().__init__()\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(2, 32, 3, stride=1, padding=1), # shape: [32,patch_size,patch_size]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=1, padding=1), # shape: [64,patch_size,patch_size] \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1), # shape: [128,patch_size,patch_size] \n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*patch_size*patch_size, 1000),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(1000, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = self.conv_stack(x)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20382530",
   "metadata": {},
   "source": [
    "Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = CNN_s1(patch_size=patch_size).to(device)\n",
    "optimizer = torch.optim.Adam(model_cnn.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss_hist, loss_hist = train(model_cnn, train_loader, valid_loader, criterion, optimizer, num_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d537ae",
   "metadata": {},
   "source": [
    "Визуализация изменения лоссов от эпохи к эпохе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(10), loss_hist)\n",
    "plt.plot(range(10), val_loss_hist)\n",
    "plt.legend(['Train loss', 'Val loss'])\n",
    "plt.xlabel(\"Epochs\", fontsize=15)\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00ab6ac",
   "metadata": {},
   "source": [
    "Валидация модели на валидационном наборе. Оценка качества модели и сборка результирующего изображения в блокноте, посвященном тестированию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caba84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, __ = final_validate(model_cnn, \n",
    "                       criterion,\n",
    "                       valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0d785f",
   "metadata": {},
   "source": [
    "Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_cnn, \"/content/model_cnn.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f96e09",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "\n",
    "Поскольку оригинальные версии ResNet достаточно глубокие и содержат несколько слоев пулинга, они не подходят нам для нашей задачи с маленькими патчами. Тем не менее, архитектуру с пробрасыванием можно попробовать использовать, поэтому была написана мини-версия ResNet с 7 слоями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e095204",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomResnet(nn.Module):\n",
    "    def __init__(self, class_nums = 1, patch_size=5):\n",
    "        super(CustomResnet, self).__init__()\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        resnet_module = nn.Sequential(\n",
    "            nn.Conv2d(2, 64, 3, stride=1, padding=1),\n",
    "            self.activation,\n",
    "            BasicBlock(64, 64, 1),\n",
    "            BasicBlock(64, 128, 2),\n",
    "            BasicBlock(128, 128, 1),\n",
    "            nn.AdaptiveAvgPool2d((1,1))\n",
    "        )\n",
    "\n",
    "        dummy_imput = torch.rand(1, 2, patch_size, patch_size)\n",
    "        out = resnet_module(dummy_imput)\n",
    "\n",
    "        self.resnet = resnet_module\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(out.shape[1], class_nums),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = nn.Flatten()(x)\n",
    "        scores = self.fc(x)\n",
    "        return scores\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, conv_in, conv_out, stride_first, activation = nn.ReLU):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.activation = activation()\n",
    "\n",
    "        if stride_first == 2:\n",
    "            downs_module = nn.Sequential(\n",
    "            nn.Conv2d(conv_in, conv_out, 1, stride=2),\n",
    "            nn.BatchNorm2d(conv_out)\n",
    "            )\n",
    "            self.downsample = downs_module\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "        bb_module = nn.Sequential(\n",
    "            nn.Conv2d(conv_in, conv_out, 3, stride=stride_first, padding=1),\n",
    "            nn.BatchNorm2d(conv_out),\n",
    "            self.activation,\n",
    "            nn.Conv2d(conv_out, conv_out, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(conv_out)\n",
    "        )\n",
    "        self.bb = bb_module\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_identity = x\n",
    "        out = self.bb(x)\n",
    "\n",
    "        if self.downsample is not None: \n",
    "            x_identity = self.downsample(x) \n",
    "\n",
    "        out += x_identity\n",
    "        out = self.activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5ce6b",
   "metadata": {},
   "source": [
    "Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4437d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = CustomResnet(patch_size=patch_size).to(device)\n",
    "optimizer = torch.optim.Adam(model_resnet.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss_hist, loss_hist = train(model_resnet, train_loader, valid_loader, criterion, optimizer, num_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea29c657",
   "metadata": {},
   "source": [
    "Визуализация изменения лоссов от эпохи к эпохе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eb19cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(10), loss_hist)\n",
    "plt.plot(range(10), val_loss_hist)\n",
    "plt.legend(['Train loss', 'Val loss'])\n",
    "plt.xlabel(\"Epochs\", fontsize=15)\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749eaf9c",
   "metadata": {},
   "source": [
    "Валидация модели на валидационном наборе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, __ = final_validate(model_resnet, \n",
    "                        criterion,\n",
    "                        valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816dafd2",
   "metadata": {},
   "source": [
    "Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494f289",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_resnet, '/content/model_resnet.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4905f",
   "metadata": {},
   "source": [
    "## Сегментация сверточной сетью U-Net\n",
    "\n",
    "Поскольку модель сегментирует, а не классифицирует патчи, для нее необходимы отдельные класс для создания патчей и функции для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022496e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# генератор патчей\n",
    "class Patcher_UNet(Dataset):\n",
    "    def __init__(self, image, mask, transform, patch_size=256, train_part=True):\n",
    "        super().__init__()\n",
    "              \n",
    "        self.image = image\n",
    "        self.mask = mask\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.im_h, self.im_w = image.shape[0], image.shape[1]\n",
    "        self.train_part = train_part\n",
    "    \n",
    "        coord_list = list()\n",
    "        # сохраняем координаты верхнего левого угла каждого патча для удобства восстановления маски\n",
    "        for corner_x in trange(0, self.im_w // self.patch_size * self.patch_size, self.patch_size): \n",
    "            for corner_y in range(0,  self.im_h // self.patch_size * self.patch_size, self.patch_size):\n",
    "                if (self.image[corner_y:corner_y + self.patch_size,\n",
    "                               corner_x:corner_x + self.patch_size] != 0).all():\n",
    "                    coord_list.append([corner_y, corner_x])\n",
    "        if not train_part:\n",
    "            corner_x = self.im_w - self.patch_size\n",
    "            for corner_y in range(0,  self.im_h // self.patch_size * self.patch_size, self.patch_size):\n",
    "                coord_list.append([corner_y, corner_x])\n",
    "            \n",
    "        self.coords = np.array(coord_list)\n",
    "        self.size = len(self.coords)\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        corner_x = self.coords[indx, 1]\n",
    "        corner_y = self.coords[indx, 0]\n",
    "        \n",
    "        patch = self.image[corner_y:corner_y + self.patch_size, \n",
    "                           corner_x:corner_x + self.patch_size]\n",
    "        label = self.mask[corner_y:corner_y + self.patch_size, \n",
    "                           corner_x:corner_x + self.patch_size]\n",
    "        \n",
    "        if self.train_part:\n",
    "            trans = transforms.Compose([transforms.ToTensor()])\n",
    "            concat = torch.cat((trans(patch), trans(label)))\n",
    "            concat_transformed = self.transform(concat)\n",
    "            patch, label = torch.split(concat_transformed, 2)\n",
    "        else:\n",
    "            patch, label = self.transform(patch), self.transform(label)\n",
    "        return patch, label, indx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878c12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для валидации\n",
    "def validate_unet(model,\n",
    "                  criterion,\n",
    "                  val_loader):\n",
    "    cumloss = 0\n",
    "    loss_history = []\n",
    "    mcc = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            imgs, labels, coords = batch\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            y_pred = model(imgs) # get predictions\n",
    "            loss = criterion(y_pred.squeeze(), labels.squeeze()) # compute loss\n",
    "            loss_history.append(loss.cpu().detach().numpy()) # write loss to log\n",
    "            cumloss += loss\n",
    "            y_pred = y_pred.squeeze().cpu()\n",
    "            labels = labels.squeeze().cpu()\n",
    "            y_pred = torch.where(y_pred > 0.5, 1, 0).to(torch.int8)\n",
    "            mcc_batch = matthews_corrcoef(torch.flatten(labels), torch.flatten(y_pred))\n",
    "            mcc.append(mcc_batch)\n",
    "    return cumloss / len(val_loader), loss_history, np.mean(mcc) # mean loss and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f9ff6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для обучения\n",
    "def train_unet(model, train_data, test_data, criterion, optimizer, num_epochs=10):\n",
    "    loss_hist = []\n",
    "    val_loss_lst = []\n",
    "    epochs = trange(num_epochs)\n",
    "    for epoch in epochs:\n",
    "        ep_loss = 0\n",
    "        model.train()\n",
    "        for batch in train_data:\n",
    "            imgs, labels, coords = batch\n",
    "            imgs, labels = imgs.to(device), labels.to(device)     \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            ep_loss += loss.item()\n",
    "        loss_hist.append(ep_loss / len(train_data))\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, val_loss_hist, mcc = validate_unet(model, criterion, test_data)\n",
    "        val_loss_lst.append(val_loss.cpu())\n",
    "        \n",
    "        if not epoch % 50:\n",
    "            print(f\"Epoch={epoch}  loss={loss_hist[epoch]:.4}  val_loss={val_loss:.4}  mcc={mcc:.2}\")\n",
    "    return val_loss_lst, loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27078a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean=0., std=1.):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950962e2",
   "metadata": {},
   "source": [
    "Поскольку U-Net классифицирует целые патчи, а не пикселы, необходимо увеличить объем тренировочной части, но незначительно, чтобы модели были сопоставимы. Для удобства сделаем границы кратными размеру патча"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b7012",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 256\n",
    "\n",
    "bound_train = 1000 \n",
    "bound_val = bound_train + patch_size*5\n",
    "bound_test = bound_val + patch_size*3\n",
    "print(f\"bound_val={bound_val}  bound_test={bound_test}\")\n",
    "\n",
    "\n",
    "train_image = image_norm[bound_train:bound_val]\n",
    "train_labels = mask_array[bound_train:bound_val]\n",
    "\n",
    "val_image = image_norm[bound_val:bound_test]\n",
    "val_labels = mask_array[bound_val:bound_test]\n",
    "\n",
    "print(train_image.shape)\n",
    "print(val_image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb5c356",
   "metadata": {},
   "source": [
    "Так как патчи в случае с сегментацией состоят из множества пикселов, то патчей будет получаться значительно меньше, чем в случае попиксельной классификации сверточными сетями. Поэтому возникает необходимость в увеличении разнообразия аугментации.\n",
    "\n",
    "Для аугментации, помимо шума, были выбраны случайный поворот, зеркальное отражение и вырезание случайного фрагмента патча."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbc2b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=(0, 180)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomCrop(128),\n",
    "    AddGaussianNoise(0., 0.2)])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4172d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new loader\n",
    "train_dataset = Patcher_UNet(train_image, train_labels, train_transform, patch_size, train_part = True)\n",
    "valid_dataset = Patcher_UNet(val_image, val_labels, valid_transform, patch_size, train_part = False)\n",
    "print(len(train_dataset))\n",
    "print(len(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05bf028",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf8e515",
   "metadata": {},
   "source": [
    "В качестве функции потерь выбран Binary Dice Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a521c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "class BinaryDiceLoss(nn.Module):\n",
    "    def __init__(self, p=2, epsilon=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = p  # pow degree\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, predict, target):\n",
    "        predict = predict.flatten(1)\n",
    "        target = target.flatten(1)\n",
    "\n",
    "        num = torch.sum(torch.mul(predict, target), dim=1) + self.epsilon\n",
    "        den = torch.sum(predict.pow(self.p) + target.pow(self.p), dim=1) + self.epsilon\n",
    "        loss = 1 - 2 * num / den\n",
    "\n",
    "        return loss.mean()  # over batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad151868",
   "metadata": {},
   "source": [
    "Возьмем оригинальную U-Net, но нам вряд ли пригодятся предобученные веса из-за специфики наших данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc28428",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unet = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "                       in_channels=2, out_channels=1, init_features=32, pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0826b62",
   "metadata": {},
   "source": [
    "Обучение. Поскольку U-Net учится значительно быстрее сверточных сетей из первого раздела, количество эпох было значительно увеличено"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef48098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "model_unet = model_unet.to(device)\n",
    "optimizer = torch.optim.Adam(model_unet.parameters(), lr=0.001)\n",
    "criterion = BinaryDiceLoss()\n",
    "val_loss_hist, loss_hist = train_unet(model_unet, train_loader, valid_loader, criterion, optimizer, num_epochs = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d9a0d",
   "metadata": {},
   "source": [
    "Визуализация изменения лоссов от эпохи к эпохе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef81f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(500), loss_hist)\n",
    "plt.plot(range(500), val_loss_hist)\n",
    "plt.legend(['Train loss', 'Val loss'])\n",
    "plt.xlabel(\"Epochs\", fontsize=15)\n",
    "plt.ylabel(\"Loss\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9cf6e6",
   "metadata": {},
   "source": [
    "Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0d217",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_unet, \"/content/model_unet.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c2edf",
   "metadata": {},
   "source": [
    "## Классификация при помощи Random Forest\n",
    "\n",
    "Последняя модель - представитель методов классического машинного обучения\n",
    "\n",
    "Поскольку мы хотим, чтобы тренировочный набор был перемешан, сперва отберем из снимка весь участок для тренировки и валидации, который использовался для сверточных моделей для классификации патчей, и уже из него случайным образом отберем тренировочные и валидационные части."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_train = 1500\n",
    "bound_val = 2500\n",
    "bound_test = 2700\n",
    "\n",
    "\n",
    "train_val_image = image_norm[bound_train:bound_test]\n",
    "train_val_labels = mask_array[bound_train:bound_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417105f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_image = np.reshape(train_val_image, \n",
    "                             [train_val_image.shape[0] * train_val_image.shape[1], train_val_image.shape[2]])\n",
    "train_val_labels = train_val_labels.flatten()\n",
    "print(train_val_image.shape)\n",
    "print(train_val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee332665",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_rf, val_image_rf, train_labels_rf, val_labels_rf = train_test_split(\n",
    "                                                               train_val_image, train_val_labels, \n",
    "                                                               test_size=(bound_test - bound_val)/(bound_test - bound_train), \n",
    "                                                               random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0502de5",
   "metadata": {},
   "source": [
    "Обучение модели. Количество и глубина деревьев ограничены, чтобы помещаться в памяти."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c231f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=200, max_depth = 25, n_jobs=4, verbose=2)\n",
    "\n",
    "rf = rf.fit(train_image_rf, train_labels_rf)\n",
    "\n",
    "y_pred_val = rf.predict(val_image_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7dedf4",
   "metadata": {},
   "source": [
    "Оценка на валидационной части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e9118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Matthews correlation coefficient={matthews_corrcoef(val_labels_rf, y_pred_val):.2}\")\n",
    "print(f\"ROC_AUC={roc_auc_score(val_labels_rf, y_pred_val):.2}\")\n",
    "print(f\"Balanced accuracy score={balanced_accuracy_score(val_labels_rf, y_pred_val):.2}\")\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(val_labels_rf, y_pred_val).ravel()\n",
    "iou = IoU(tp, fp, fn)\n",
    "\n",
    "print(f\"Intersection over Union = {iou:.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2862f1",
   "metadata": {},
   "source": [
    "Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32f86b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(rf, \"/content/random_forest.joblib\") # very heavy model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envpytorch",
   "language": "python",
   "name": "envpytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
